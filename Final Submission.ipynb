{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75a14f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jonathan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jonathan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jonathan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Jonathan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f8635d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"C:/Users/Jonathan/Downloads/south-african-language-identification-hack-2023/train_set.csv\")\n",
    "test_set = pd.read_csv(\"C:/Users/Jonathan/Downloads/south-african-language-identification-hack-2023/test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b6ac451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "   \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "  \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    " \n",
    "    tokens = word_tokenize(text)\n",
    "   \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "    \n",
    "    return lemmatized_output\n",
    "\n",
    "\n",
    "train_set['cleaned_text'] = train_set['text'].apply(preprocess_text)\n",
    "test_set['cleaned_text'] = test_set['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "229f276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X = tfidf.fit_transform(train_set['cleaned_text'])\n",
    "y = train_set['lang_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae65e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "cv_scores = cross_val_score(lr_model, X, y, cv=5)\n",
    "cv_mean = cv_scores.mean()\n",
    "cv_std = cv_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5c72453",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_features = tfidf.transform(test_set['cleaned_text'])\n",
    "\n",
    "predictions = lr_model.predict(test_set_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "393ff1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'index': test_set['index'],\n",
    "    'lang_id': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0725e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
